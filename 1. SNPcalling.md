# SNP calling

## Quality control

Data from the two plates is in different folders:

plate_1: 'SQ1807_HY5HNDRXY_s_1_fastq.txt.gz', 'SQ1807_HY5HNDRXY_s_2_fastq.txt.gz' and keyfile 'key.txt'

plate_2: 'SQ1875_HNFTMDRXY_s_1_fastq.txt.gz', 'SQ1875_HNFTMDRXY_s_2_fastq.txt.gz' and keyfile 'key.txt'

Have a quick look at quality

```
module load FastQC
```

Plate 1:

```
zcat SQ1807_HY5HNDRXY_s_1_fastq.txt.gz | head -n 1000000 > lane1.fq
```

```
zcat SQ1807_HY5HNDRXY_s_2_fastq.txt.gz | head -n 1000000 > lane2.fq
```

```
fastqc *fq
```

Plate 2:

```
zcat SQ1875_HNFTMDRXY_s_1_fastq.txt.gz | head -n 1000000 > lane1.fq
```

```
zcat SQ1875_HNFTMDRXY_s_2_fastq.txt.gz | head -n 1000000 > lane2.fq
```

```
fastqc *fq
```

Outputs fastqc.html files that i can now look at, looks okay but adapters need to be removed. 
Remove common Illumina adapter used by AgResearch:

```
module load cutadapt
```

plate 1:

```
cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -m 30 -o SQ1807_HY5HNDRXY_s_1_cleaned.fastq SQ1807_HY5HNDRXY_s_1_fastq.txt.gz
```

```
cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -m 30 -o SQ1807_HY5HNDRXY_s_2_cleaned.fastq SQ1807_HY5HNDRXY_s_2_fastq.txt.gz
```

plate 2:

```
cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -m 30 -o SQ1875_HNFTMDRXY_s_1_cleaned.fastq SQ1875_HNFTMDRXY_s_1_fastq.txt.gz
```

```
cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -m 30 -o SQ1875_HNFTMDRXY_s_2_cleaned.fastq SQ1875_HNFTMDRXY_s_2_fastq.txt.gz
```

Run FastQC again on the cleaned files:

```
fastqc SQ1807_HY5HNDRXY_s_1_cleaned.fastq
```
```
fastqc SQ1807_HY5HNDRXY_s_2_cleaned.fastq
```
```
fastqc SQ1875_HNFTMDRXY_s_1_cleaned.fastq
```
```
fastqc SQ1875_HNFTMDRXY_s_2_cleaned.fastq
```

## Demultiplexing
 
I will create additional directories in /plate_1 and /plate_2, one for the raw data, one for all samples (here I will eventually find one file per sample with all the reads for that individual) and one containing the source file (i.e., the cleaned files)

in /plate_1:
 
```
mkdir rawSQ1807 samplesSQ1807 source_files
```

in /plate_2:

```
mkdir rawSQ1875 samplesSQ1875 source_files
```

Move all cleaned files into /source_files

plate 1:
```
mv SQ1807*cleaned* ./source_files
```

plate 2:
```
mv SQ1875*cleaned* ./source_files
```

Then go into raw folders and link raw datafile in here:

plate 1:
```
cd ./rawSQ1807
```
```
ln -s ../source_files/SQ1807*cleaned* . 
```

plate 2:

```
cd ./rawSQ1875
```
```
ln -s ../source_files/SQ1875*cleaned* . 
```

I have created a barcode file for the samples on each plate using the key.txt file provided by AgResearch. I matched each barcode (e.g., 'ATACTTGTG') to the original sample ID (e.g. 'MA001'). The barcode files are named barcodes_SQ1807.txt and barcodes_SQ1875.txt and have been uploaded into the directories /plate_1/source_files and /plate_2_source_files.

Now I'm ready to run Stacks process_radtags

```
module load Stacks
```
```
process_radtags -p rawSQ1807 -b source_files/barcodes_SQ1807.txt --renz_1 pstI --renz_2 mspI -o samplesSQ1807 -c -q -r --inline_null
```
```
process_radtags -p rawSQ1875 -b source_files/barcodes_SQ1875.txt --renz_1 pstI --renz_2 mspI -o samplesSQ1875 -c -q -r --inline_null
```

I can then add the individual file *IDname*.fq.gz file from each plate together into a new folder, named /allsamples/samples_all:

```
cp plate_1/samplesSQ1807/*fq.gz allsamples/samples_all/
cp plate_2/samplesSQ1875/*fq.gz allsamples/samples_all/
```

As two samples (1R and 2R) were sequenced twice, I will combine the data from each plat for these samples:

```
zcat allsamples/samples_all/1R.fq.gz plate_2/samplesSQ1875/R1.fq.gz | gzip -c > allsamples/samples_all/1R_new.fq.gz
```

Side notes: 
- As I have already moved the original 1R.fq.gz and 2R.gq.gz into /samples_all, i need to give the new file a new temporary name (e.g., 1R_new.fq.gz) and then change the name afterwards after I have renamed the original file). 
- on first plate I named samples 1R and 2R, on second plate I accidentally did R1 and R2 so make sure I get the right one for the command, no need to delete the old ones from samples_all (i.e. R1 and R2 as long as they are not on popmap it doesn't matter).


In the /allsamples folder I will also create two more directories for the downstream analyses:

```
mkdir refmap_output
mkdir samples_mapped
```


## Alignment and variant calling

The mohua reference genome was downloaded from NCBI and named mohua_ref_genome.fna. It's in the /ref_genome folder

Have a quick look at ref genome:

```
head -n 10 mohua_ref_genome.fna
```
For the next steps I will use the softwares BWA (a good software for mapping, bwa mem is an algorithm to align short reads to a reference genome) and Samtools (a software that can interact with the sam/bam alignment format, e.g. sorting, merging etc.). I  need to index the reference genome, this creates a map of the genome so that BWA can navigate it better/faster and does not have to read the entire genome every time/for each read

```
module spider BWA
module load BWA
module spider samtools
module load SAMtools
```

```
bwa index mohua_ref_genome.fna
```

Now I can map my reads to the mohua reference genome using a for loop (do this as a job):

```
for filename in samples_all/*fq.gz
do base=$(basename ${filename} .fq.gz)
echo $base
bwa mem -t 8 ../ref_genome/mohua_ref_genome.fna samples_all/${base}.fq.gz | samtools view -b | samtools sort --threads 4 > samples_mapped/${base}.bam
done
```

Basically, each sample in my samples_all folder ends with .fq.gz (but there's also a .log file in there). So for each of those .fq.gz samples I will use the base function to take the filename only (the prefix, without .fq.gz), I will then reprint those using echo. Then i will use bwa and samtools to align each sample to the reference genome (with bwa mem), then pipe the output into samtools view -b to create a bam file. This bam file output is then piped into the samtools sort command and finally outputted into my samples_mapped folder with as filename.bam

Next step is to run the ref_map pipeline, for this I need to upload my popmap.

Running ref_map.pl (I will run this as a job, but will try whether the command  works and then cancel it (control+c) :

```
ref_map.pl --samples samples_mapped/ -o refmap_output/ --popmap popmap184inds.txt -T 8
```

I can now run populations without any filtering to get a vcf file, which I can then use to mask the reference genome. 

```
mkdir populations_output
```
```
populations -P refmap_output/ -M popmap184inds.txt --vcf -O populations_output
```

---

Once reference genome has been masked, I can use it to repeat the SNP calling:

```
module load BWA
module load SAMtools
```

```
bwa index masked_ref_genome.fasta
```

Now I can map my reads to the mohua reference genome using a for loop (do this as a job):

```
for filename in samples_all/*fq.gz
do base=$(basename ${filename} .fq.gz)
echo $base
bwa mem -t 8 ../ref_genome/masked_ref_genome.fna samples_all/${base}.fq.gz | samtools view -b | samtools sort --threads 4 > samples_mapped_maskedgenome/${base}.bam
done
```

Running ref_map.pl again (as job): 

```
ref_map.pl --samples samples_mapped_maskedgenome/ -o refmap_output_maskedgenome/ --popmap popmap184inds.txt -T 8
```

Run populations again:

```
populations -P refmap_output_maskedgenome/ -M popmap184inds.txt -O populations_output_maskedgenome --vcf --plink --structure --genepop --phylip
```


```
populations -P refmap_output_maskedgenome/ -M popmap184inds.txt -O populations_output_maskedgenome/write-single-SNPs --write-single-snp --vcf
```
